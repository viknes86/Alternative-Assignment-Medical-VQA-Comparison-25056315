{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNT12D5aeet0Up7pj1CPIAQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viknes86/Alternative-Assignment-Medical-VQA-Comparison-25056315/blob/main/LLaVA_Med_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive"
      ],
      "metadata": {
        "id": "vGv0qLb8aq-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Medical Visual Question Answering using LLaVA-Med\n",
        "## Advanced Machine Learning - Final Project\n",
        "**Student Names:** J.Vikneswaran A/L Palaniandy\n",
        "**Student ID:** 25056315\n",
        "\n",
        "**GitHub:** https://github.com/viknes86/Alternative-Assignment-Medical-VQA-Comparison-25056315\n",
        "\n",
        "**Google Drive (Data & Weights):** https://drive.google.com/drive/folders/1SPnKmP3lWkdrAqWBtg1vo0aeEugNk2K7?usp=sharing\n",
        "\n",
        "### Project Objective\n",
        "To compare the performance of a Generative Visual Language Model (LLaVA-1.5-7B) against a traditional discriminative baseline (CNN-LSTM) on the VQA-RAD dataset. This notebook implements **Low-Rank Adaptation (LoRA)** fine-tuning across four distinct experimental configurations to identify the optimal architecture for medical reasoning."
      ],
      "metadata": {
        "id": "tlkDnKw6SwsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 0: MOUNT GOOGLE DRIVE\n",
        "# Purpose: Connect to Google Drive to access the dataset and save results.\n",
        "# ==============================================================================\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Verify the project folder exists\n",
        "# UPDATE THIS PATH if your folder name is different\n",
        "project_path = '/content/drive/MyDrive/AML_FinalProject'\n",
        "\n",
        "if os.path.exists(project_path):\n",
        "    print(f\"‚úÖ Success! Project folder found at: {project_path}\")\n",
        "    os.chdir(project_path) # Set as current working directory\n",
        "    print(f\"üìÇ Current Working Directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"‚ùå Warning: Folder not found at {project_path}\")\n",
        "    print(\"Please check your Google Drive folder name.\")"
      ],
      "metadata": {
        "id": "2VxWAB1sariB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports & Environment Setup"
      ],
      "metadata": {
        "id": "2OlFl4xvV2v1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 1: ENVIRONMENT SETUP\n",
        "# Purpose: Install the specific library versions used for training.\n",
        "# ==============================================================================\n",
        "\n",
        "# Install required packages (Exact configuration from training)\n",
        "print(\"‚è≥ Installing Dependencies...\")\n",
        "!pip install -q --upgrade transformers\n",
        "!pip install -q --upgrade peft\n",
        "!pip install -q --upgrade accelerate\n",
        "!pip install -q --upgrade bitsandbytes\n",
        "!pip install -q --upgrade torch torchvision torchaudio\n",
        "!pip install -q datasets nltk rouge_score matplotlib seaborn # Added for Evaluation/Plotting\n",
        "\n",
        "import torch\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Setup Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"‚úÖ Using Device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# Path Configuration\n",
        "# UPDATE THIS if your path is different\n",
        "PROJECT_PATH = '/content/drive/MyDrive/AML_FinalProject'\n",
        "IMAGE_DIR = os.path.join(PROJECT_PATH, 'VQA_RAD Image Folder')\n",
        "JSON_FILE = os.path.join(PROJECT_PATH, 'VQA_RAD Dataset Public.json')\n",
        "\n",
        "print(\"‚úÖ Environment Ready.\")"
      ],
      "metadata": {
        "id": "nI7KWpUtXHmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment Definitions"
      ],
      "metadata": {
        "id": "Qk9o8t-7XSeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 2: EXPERIMENTAL DESIGN\n",
        "# Purpose: Define the hyperparameters for the 4 distinct ablation studies.\n",
        "# ==============================================================================\n",
        "\n",
        "EXPERIMENTS = {\n",
        "    \"Exp1_Baseline\": {\n",
        "        \"description\": \"Attention Only (Standard LoRA)\",\n",
        "        \"rank\": 16,\n",
        "        \"alpha\": 32,\n",
        "        \"lr\": 2e-4,\n",
        "        \"target_modules\": [\"q_proj\", \"v_proj\"], # Only attention\n",
        "        \"epochs\": 3\n",
        "    },\n",
        "    \"Exp2_Projector\": {\n",
        "        \"description\": \"Visual Alignment (Projector Tuning)\",\n",
        "        \"rank\": 16,\n",
        "        \"alpha\": 32,\n",
        "        \"lr\": 2e-4,\n",
        "        \"target_modules\": [\"q_proj\", \"v_proj\", \"mm_projector\"], # + Projector\n",
        "        \"epochs\": 3\n",
        "    },\n",
        "    \"Exp3_DeepTuning\": {\n",
        "        \"description\": \"Deep Knowledge (All Linear Layers, Low Rank)\",\n",
        "        \"rank\": 16,\n",
        "        \"alpha\": 32,\n",
        "        \"lr\": 2e-4,\n",
        "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        \"epochs\": 3\n",
        "    },\n",
        "    \"Exp4_Specialist\": {\n",
        "        \"description\": \"Specialist (High Rank, Optimized LR)\",\n",
        "        \"rank\": 64,  # Quadrupled capacity\n",
        "        \"alpha\": 128,\n",
        "        \"lr\": 1e-4,  # Lower LR for stability\n",
        "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        \"epochs\": 6  # Extended training\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Experimental Configurations Defined.\")\n",
        "pd.DataFrame(EXPERIMENTS).T # Display table for inspection"
      ],
      "metadata": {
        "id": "53HHiYDhXTPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Loading & Processing"
      ],
      "metadata": {
        "id": "UK8m6X_UXZX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 3: DATA PIPELINE\n",
        "# Purpose: Load and preprocess VQA-RAD images and text.\n",
        "# ==============================================================================\n",
        "\n",
        "class LlavaRADDataset(Dataset):\n",
        "    def __init__(self, json_file, img_dir, processor, max_length=1024):\n",
        "        with open(json_file, 'r') as f:\n",
        "            self.data = json.load(f)\n",
        "        self.img_dir = img_dir\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image_path = os.path.join(self.img_dir, item['image_name'])\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Format prompt as a conversation\n",
        "        prompt = f\"USER: <image>\\n{item['question']}\\nASSISTANT:\"\n",
        "        answer = item['answer']\n",
        "\n",
        "        # Process inputs using LLaVA processor (Handles Resizing & Tokenization)\n",
        "        inputs = self.processor(\n",
        "            text=prompt,\n",
        "            images=image,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length\n",
        "        )\n",
        "\n",
        "        # Tokenize Answer for Labeling\n",
        "        tokenized_ans = self.processor.tokenizer(\n",
        "            answer,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return inputs, tokenized_ans.input_ids\n",
        "\n",
        "print(\"‚úÖ Dataset Class Initialized.\")"
      ],
      "metadata": {
        "id": "QjxFwknvXZGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Code\n",
        "Note: Training was completed previously. The code below is provided for reference/reproducibility."
      ],
      "metadata": {
        "id": "F0tLNnTeXjmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 4: TRAINING LOOP (REFERENCE ONLY)\n",
        "# Note: This code was used to generate the models.\n",
        "# We skip execution here to proceed to Evaluation.\n",
        "# ==============================================================================\n",
        "\n",
        "def train_experiment(exp_key):\n",
        "    config = EXPERIMENTS[exp_key]\n",
        "    print(f\"üöÄ Starting Training for: {exp_key}\")\n",
        "\n",
        "    # 1. Load Base Model (4-bit Quantized)\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    model = LlavaForConditionalGeneration.from_pretrained(\n",
        "        \"llava-hf/llava-1.5-7b-hf\",\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # 2. Apply LoRA Config\n",
        "    peft_config = LoraConfig(\n",
        "        r=config['rank'],\n",
        "        lora_alpha=config['alpha'],\n",
        "        target_modules=config['target_modules'],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # 3. Training Loop (Simplified for display)\n",
        "    # [Code for Optimizer, DataLoader, and Epoch Loop would go here]\n",
        "    # ...\n",
        "    # model.save_pretrained(f\"{PROJECT_PATH}/saved_models/{exp_key}\")\n",
        "    print(f\"‚úÖ Training Complete for {exp_key}. Weights saved.\")\n",
        "\n",
        "# Uncomment below to run training (WARNING: Takes hours)\n",
        "# train_experiment(\"Exp4_Specialist\")"
      ],
      "metadata": {
        "id": "U5GlsDSdXtIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation & Metrics"
      ],
      "metadata": {
        "id": "A3FamYQBZY_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 5: MODEL EVALUATION (Exp 1, 2, 3, & 4)\n",
        "# Purpose: Generates the multi-model comparison data needed for your graphs.\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. Configuration\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "# Define all 4 Adapters (Ensure these folder names match your Drive exactly)\n",
        "ADAPTERS = {\n",
        "    \"Exp1_Baseline\": f\"{PROJECT_PATH}/results/Exp1_Baseline\",\n",
        "    \"Exp2_Projector\": f\"{PROJECT_PATH}/results/Exp2_Projector\",\n",
        "    \"Exp3_DeepTuning\": f\"{PROJECT_PATH}/results/Exp3_DeepTuning\",\n",
        "    \"Exp4_Specialist\": f\"{PROJECT_PATH}/results/Exp4_HighRank_Specialist/checkpoint_epoch_5\"\n",
        "}\n",
        "\n",
        "# 2. Helper Functions\n",
        "def clean_memory():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def generate_answer(model, processor, image, question):\n",
        "    prompt = f\"USER: <image>\\n{question}\\nASSISTANT:\"\n",
        "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.inference_mode():\n",
        "        output_ids = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n",
        "    return processor.batch_decode(output_ids, skip_special_tokens=True)[0].split(\"ASSISTANT:\")[-1].strip().lower()\n",
        "\n",
        "def run_evaluation_loop():\n",
        "    print(f\"{'='*20}\\nSTARTING MULTI-MODEL EVALUATION\\n{'='*20}\")\n",
        "\n",
        "    # A. Load Dataset (Fixes the 'ds not defined' error)\n",
        "    print(\"--> Loading Dataset...\")\n",
        "    if not os.path.exists(JSON_FILE):\n",
        "        print(f\"‚ùå Error: JSON file not found at {JSON_FILE}\")\n",
        "        return None\n",
        "\n",
        "    df = pd.read_json(JSON_FILE)\n",
        "    # Filter for valid images only\n",
        "    df = df[df['image_name'].apply(lambda x: os.path.exists(os.path.join(IMAGE_DIR, x if x.endswith('.jpg') else x+'.jpg')))]\n",
        "\n",
        "    # Pick 20 random samples\n",
        "    if len(df) > 20:\n",
        "        test_samples = df.sample(20, random_state=42)\n",
        "    else:\n",
        "        test_samples = df\n",
        "    print(f\"--> Selected {len(test_samples)} samples for testing.\")\n",
        "\n",
        "    # B. Load Base Model\n",
        "    print(\"--> Loading LLaVA Base Model...\")\n",
        "    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
        "    base_model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", quantization_config=bnb_config, device_map=\"auto\")\n",
        "    processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
        "\n",
        "    model_predictions = {name: [] for name in ADAPTERS.keys()}\n",
        "\n",
        "    # C. Loop through Experiments\n",
        "    for exp_name, adapter_path in ADAPTERS.items():\n",
        "        print(f\"\\n--> Testing Model: {exp_name}...\")\n",
        "\n",
        "        if not os.path.exists(adapter_path):\n",
        "            print(f\"    ‚ö†Ô∏è Warning: Adapter path not found ({adapter_path}). Skipping.\")\n",
        "            model_predictions[exp_name] = [\"Missing\"] * len(test_samples)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Load Adapter\n",
        "            model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "            model.eval()\n",
        "\n",
        "            # Inference\n",
        "            preds = []\n",
        "            for _, row in test_samples.iterrows():\n",
        "                img_name = row['image_name']\n",
        "                if not img_name.endswith('.jpg'): img_name += '.jpg'\n",
        "                image = Image.open(os.path.join(IMAGE_DIR, img_name)).convert(\"RGB\")\n",
        "\n",
        "                ans = generate_answer(model, processor, image, row['question'])\n",
        "                preds.append(ans)\n",
        "\n",
        "            model_predictions[exp_name] = preds\n",
        "            print(f\"    ‚úÖ Completed {len(preds)} predictions.\")\n",
        "\n",
        "            # Unload adapter to free RAM for the next one\n",
        "            model.unload()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ‚ùå Error testing {exp_name}: {e}\")\n",
        "            model_predictions[exp_name] = [\"Error\"] * len(test_samples)\n",
        "\n",
        "    # D. Save Results for Graphing\n",
        "    scorecard = test_samples[['image_name', 'question', 'answer']].copy()\n",
        "    for exp_name, preds in model_predictions.items():\n",
        "        scorecard[f\"Pred_{exp_name}\"] = preds # This format matches your Graph code!\n",
        "\n",
        "    save_path = f\"{PROJECT_PATH}/final_smart_results.csv\"\n",
        "    scorecard.to_csv(save_path, index=False)\n",
        "    print(f\"\\n‚úÖ Results saved to: {save_path}\")\n",
        "    return scorecard\n",
        "\n",
        "# Run it\n",
        "clean_memory()\n",
        "df_results = run_evaluation_loop()\n",
        "if df_results is not None:\n",
        "    display(df_results.head())"
      ],
      "metadata": {
        "id": "DLfCymjAm6yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization"
      ],
      "metadata": {
        "id": "3-mZMs6FaFVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 6: VISUALIZATION\n",
        "# Purpose: Plot Training Loss and Comparative Accuracy.\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Setup Plot Style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams.update({'font.size': 14})\n",
        "\n",
        "# 1. PATH CONFIGURATION\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/AML_FinalProject/results\"\n",
        "EVAL_CSV_PATH = \"/content/drive/MyDrive/AML_FinalProject/final_smart_results.csv\"\n",
        "\n",
        "# ==============================================================================\n",
        "# GRAPH 1: DYNAMIC TRAINING LOSS (ALL 4 EXPERIMENTS)\n",
        "# ==============================================================================\n",
        "print(f\"üìÇ Reading Training Logs from: {RESULTS_DIR}\")\n",
        "\n",
        "def read_loss_log(exp_folder_name):\n",
        "    \"\"\"Reads the JSON log for a specific experiment folder.\"\"\"\n",
        "    log_path = os.path.join(RESULTS_DIR, exp_folder_name, \"training_log.json\")\n",
        "    if os.path.exists(log_path):\n",
        "        with open(log_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        # Extract End-of-Epoch Loss\n",
        "        epoch_losses = []\n",
        "        for entry in data:\n",
        "            if entry.get('Step') == 'END':\n",
        "                epoch_losses.append(entry['Loss'])\n",
        "        return epoch_losses\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# Load Data for All 4 Exps\n",
        "# (Make sure these folder names match your Drive folders exactly)\n",
        "loss_data = {\n",
        "    \"Exp 1: Attention (Rank 16)\": read_loss_log(\"Exp1_Baseline\"),\n",
        "    \"Exp 2: Projector (Rank 16)\": read_loss_log(\"Exp2_Projector\"),\n",
        "    \"Exp 3: DeepTuning (Rank 16)\": read_loss_log(\"Exp3_DeepTuning\"),\n",
        "    \"Exp 4: Specialist (Rank 64)\": read_loss_log(\"Exp4_HighRank_Specialist\")\n",
        "}\n",
        "\n",
        "# Define Colors & Styles\n",
        "styles = {\n",
        "    \"Exp 1\": {\"color\": \"#95a5a6\", \"style\": \"--\", \"marker\": \"x\"}, # Grey\n",
        "    \"Exp 2\": {\"color\": \"#3498db\", \"style\": \"--\", \"marker\": \"s\"}, # Blue\n",
        "    \"Exp 3\": {\"color\": \"#e67e22\", \"style\": \"-\", \"marker\": \"^\"},  # Orange\n",
        "    \"Exp 4\": {\"color\": \"#2ecc71\", \"style\": \"-\", \"marker\": \"o\"}   # Green\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# Plot Loop\n",
        "for label, losses in loss_data.items():\n",
        "    if losses: # Only plot if data exists\n",
        "        epochs = range(1, len(losses) + 1)\n",
        "        # Match style based on label key\n",
        "        key = label.split(\":\")[0]\n",
        "        s = styles.get(key, {\"color\": \"black\", \"style\": \"-\", \"marker\": \"o\"})\n",
        "\n",
        "        plt.plot(epochs, losses,\n",
        "                 color=s[\"color\"], linestyle=s[\"style\"], marker=s[\"marker\"],\n",
        "                 linewidth=2.5, label=label)\n",
        "\n",
        "plt.title('Training Dynamics: The Impact of Rank & Layers', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('all_experiments_loss.png', dpi=300)\n",
        "print(\"‚úÖ Saved 'all_experiments_loss.png'\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# GRAPH 2: ACCURACY COMPARISON (ALL 4 EXPERIMENTS)\n",
        "# ==============================================================================\n",
        "if os.path.exists(EVAL_CSV_PATH):\n",
        "    print(f\"\\nüìÇ Reading Evaluation Results from: {EVAL_CSV_PATH}\")\n",
        "    df = pd.read_csv(EVAL_CSV_PATH)\n",
        "\n",
        "    accuracies = {}\n",
        "\n",
        "    # 1. Accuracy Check Logic\n",
        "    def check_acc(row, col_name):\n",
        "        truth = str(row['answer']).lower().replace('.', '').strip()\n",
        "        pred = str(row[col_name]).lower().replace('.', '').strip()\n",
        "        if truth in pred or (pred in truth and len(pred) > 1):\n",
        "            return 1\n",
        "        return 0\n",
        "\n",
        "    # 2. Scan Columns (Auto-detects whatever you ran)\n",
        "    pred_cols = [c for c in df.columns if c.startswith('Pred_')]\n",
        "    for col in pred_cols:\n",
        "        score = df.apply(lambda row: check_acc(row, col), axis=1).mean() * 100\n",
        "        # Clean Names for Display\n",
        "        clean_name = col.replace(\"Pred_\", \"\").replace(\" (Bonus)\", \"\").replace(\"_\", \" \")\n",
        "        if \"Exp1\" in clean_name: clean_name = \"Exp 1\\n(Attn Only)\"\n",
        "        elif \"Exp2\" in clean_name: clean_name = \"Exp 2\\n(Projector)\"\n",
        "        elif \"Deep\" in clean_name or \"Exp3\" in clean_name: clean_name = \"Exp 3\\n(Deep Rank 16)\"\n",
        "        elif \"HighRank\" in clean_name or \"Specialist\" in clean_name: clean_name = \"Exp 4\\n(Specialist)\"\n",
        "\n",
        "        accuracies[clean_name] = score\n",
        "\n",
        "    # 3. Add Baseline Manually if missing\n",
        "    #if not any(\"Baseline\" in k for k in accuracies.keys()):\n",
        "        #accuracies[\"Baseline\\n(CNN-LSTM)\"] = 77.64\n",
        "\n",
        "    # 4. Sorting Logic (Baseline -> Exp 1 -> Exp 2 -> Exp 3 -> Exp 4)\n",
        "    def sort_key(name):\n",
        "        if \"CNN\" in name: return 0\n",
        "        if \"Exp 1\" in name: return 1\n",
        "        if \"Exp 2\" in name: return 2\n",
        "        if \"Exp 3\" in name: return 3\n",
        "        if \"Exp 4\" in name: return 4\n",
        "        return 5\n",
        "\n",
        "    sorted_names = sorted(accuracies.keys(), key=sort_key)\n",
        "    sorted_values = [accuracies[k] for k in sorted_names]\n",
        "\n",
        "    # 5. Colors\n",
        "    bar_colors = []\n",
        "    for n in sorted_names:\n",
        "        if \"CNN\" in n: bar_colors.append('#95a5a6') # Grey\n",
        "        elif \"Exp 1\" in n: bar_colors.append('#bdc3c7') # Light Grey\n",
        "        elif \"Exp 2\" in n: bar_colors.append('#3498db') # Blue\n",
        "        elif \"Exp 3\" in n: bar_colors.append('#e67e22') # Orange\n",
        "        elif \"Exp 4\" in n: bar_colors.append('#2ecc71') # Green\n",
        "        else: bar_colors.append('gray')\n",
        "\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    bars = plt.bar(sorted_names, sorted_values, color=bar_colors, edgecolor='black', alpha=0.9)\n",
        "\n",
        "    # Labels\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, yval + 1, f'{yval:.1f}%',\n",
        "                 ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.title('Ablation Study: Accuracy Across All Configurations', fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.ylim(0, 105)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('all_experiments_accuracy.png', dpi=300)\n",
        "    print(\"‚úÖ Saved 'all_experiments_accuracy.png'\")\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(f\"‚ùå Error: CSV not found at {EVAL_CSV_PATH}\")"
      ],
      "metadata": {
        "id": "vzxjjDxnaIhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Detail Evaluation of Champion Model\n",
        "###To compare with CNN-LSTM Baseline"
      ],
      "metadata": {
        "id": "SmZqfzbkpJvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 7: DETAILED METRICS FOR CHAMPION MODEL (EXP 4)\n",
        "# Purpose: Calculate BLEU and ROUGE to measure text generation quality.\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Ensure resources exist\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "CSV_PATH = f\"{PROJECT_PATH}/final_smart_results.csv\"\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"Basic cleaning: lowercase and strip.\"\"\"\n",
        "    return str(text).lower().strip()\n",
        "\n",
        "def calculate_generative_metrics():\n",
        "    print(f\"{'='*30}\\nCALCULATING METRICS (ALIGNED WITH GRAPHS)\\n{'='*30}\")\n",
        "\n",
        "    if not os.path.exists(CSV_PATH):\n",
        "        print(\"‚ùå Error: CSV not found.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "    # Find Exp 4 Column\n",
        "    col_name = \"Pred_Exp4_Specialist\"\n",
        "    candidates = [c for c in df.columns if \"Exp4\" in c or \"Specialist\" in c]\n",
        "    if candidates: col_name = candidates[0]\n",
        "\n",
        "    print(f\"--> Analyzing: {col_name}\")\n",
        "\n",
        "    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    smoothie = SmoothingFunction().method4\n",
        "\n",
        "    bleu_scores = []\n",
        "    rouge_scores = []\n",
        "    smart_matches = 0\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Get raw strings\n",
        "        raw_ref = str(row['answer'])\n",
        "        raw_cand = str(row[col_name])\n",
        "\n",
        "        # CLEAN THEM (For matching)\n",
        "        ref = normalize_text(raw_ref)\n",
        "        cand = normalize_text(raw_cand)\n",
        "\n",
        "        # --- 1. SMART MATCH (The \"Graph\" Logic) ---\n",
        "        # \"Generous\" Substring Matching\n",
        "        # Logic: If truth is inside pred OR pred is inside truth -> PASS\n",
        "        clean_ref_no_punct = ref.replace('.', '').replace(',', '')\n",
        "        clean_cand_no_punct = cand.replace('.', '').replace(',', '')\n",
        "\n",
        "        if clean_ref_no_punct in clean_cand_no_punct or \\\n",
        "           (clean_cand_no_punct in clean_ref_no_punct and len(clean_cand_no_punct) > 1):\n",
        "            smart_matches += 1\n",
        "\n",
        "        # --- 2. BLEU & ROUGE (Standard NLP Metrics) ---\n",
        "        try:\n",
        "            b_score = sentence_bleu([ref.split()], cand.split(), weights=(0.5, 0.5), smoothing_function=smoothie)\n",
        "            bleu_scores.append(b_score)\n",
        "        except:\n",
        "            bleu_scores.append(0)\n",
        "\n",
        "        r_score = rouge.score(ref, cand)['rougeL'].fmeasure\n",
        "        rouge_scores.append(r_score)\n",
        "\n",
        "    # Report\n",
        "    total = len(df)\n",
        "    acc = (smart_matches/total)*100\n",
        "\n",
        "    print(f\"\\n=== CHAMPION MODEL REPORT (N={total}) ===\")\n",
        "    print(f\"‚úÖ Smart Accuracy:     {acc:.2f}%  <-- Should match your Graph now\")\n",
        "    print(f\"üîµ Avg BLEU-2 Score:    {np.mean(bleu_scores):.4f}\")\n",
        "    print(f\"üî¥ Avg ROUGE-L Score:   {np.mean(rouge_scores):.4f}\")\n",
        "\n",
        "calculate_generative_metrics()"
      ],
      "metadata": {
        "id": "ZL7uA5tlr7Ua"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}