{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMRezMQiTXyozmNtAh5GNnC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viknes86/Alternative-Assignment-Medical-VQA-Comparison-25056315/blob/main/03_Phase2_LlavaMed_Hybrid_Specialist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Medical Visual Question Answering using LLaVA-Med\n",
        "## Advanced Machine Learning - Final Project\n",
        "**Student Names:** J.Vikneswaran A/L Palaniandy\n",
        "**Student ID:** 25056315\n",
        "\n",
        "**GitHub:** https://github.com/viknes86/Alternative-Assignment-Medical-VQA-Comparison-25056315\n",
        "\n",
        "**Google Drive (Data & Weights):** https://drive.google.com/drive/folders/1SPnKmP3lWkdrAqWBtg1vo0aeEugNk2K7?usp=sharing\n",
        "\n",
        "### Project Objective\n",
        "To compare the performance of a Generative Visual Language Model (LLaVA-1.5-7B) against a traditional discriminative baseline (CNN-LSTM) on the VQA-RAD dataset. This notebook implements training on the final LLaVA model (Exp5) for the final showdown with the CNN-LSTM."
      ],
      "metadata": {
        "id": "tlkDnKw6SwsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive"
      ],
      "metadata": {
        "id": "vGv0qLb8aq-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 0: MOUNT GOOGLE DRIVE\n",
        "# Purpose: Connect to Google Drive to access the dataset and save results.\n",
        "# ==============================================================================\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Verify the project folder exists\n",
        "# UPDATE THIS PATH if your folder name is different\n",
        "project_path = '/content/drive/MyDrive/AML_FinalProject'\n",
        "\n",
        "if os.path.exists(project_path):\n",
        "    print(f\"‚úÖ Success! Project folder found at: {project_path}\")\n",
        "    os.chdir(project_path) # Set as current working directory\n",
        "    print(f\"üìÇ Current Working Directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"‚ùå Warning: Folder not found at {project_path}\")\n",
        "    print(\"Please check your Google Drive folder name.\")"
      ],
      "metadata": {
        "id": "2VxWAB1sariB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports & Environment Setup"
      ],
      "metadata": {
        "id": "2OlFl4xvV2v1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 1: ENVIRONMENT SETUP\n",
        "# Purpose: Install the specific library versions used for training.\n",
        "# ==============================================================================\n",
        "\n",
        "# Install required packages (Exact configuration from training)\n",
        "print(\"‚è≥ Installing Dependencies...\")\n",
        "!pip install -q --upgrade transformers\n",
        "!pip install -q --upgrade peft\n",
        "!pip install -q --upgrade accelerate\n",
        "!pip install -q --upgrade bitsandbytes\n",
        "!pip install -q --upgrade torch torchvision torchaudio\n",
        "!pip install -q datasets nltk rouge_score matplotlib seaborn # Added for Evaluation/Plotting\n",
        "!pip install bert_score\n",
        "!pip install evaluate rouge_score bert_score\n",
        "!pip install -U bitsandbytes accelerate peft transformers\n",
        "\n",
        "import torch\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig, TrainingArguments, Trainer\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from tqdm import tqdm\n",
        "from peft import LoraConfig, get_peft_model, PeftModel, TaskType\n",
        "from PIL import Image\n",
        "import gc\n",
        "from transformers import TrainerCallback\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score\n",
        "import evaluate\n",
        "\n",
        "\n",
        "# Setup Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"‚úÖ Using Device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# Path Configuration\n",
        "# UPDATE THIS if your path is different\n",
        "PROJECT_PATH = '/content/drive/MyDrive/AML_FinalProject'\n",
        "IMAGE_DIR = os.path.join(PROJECT_PATH, 'VQA_RAD Image Folder')\n",
        "JSON_FILE = os.path.join(PROJECT_PATH, 'VQA_RAD Dataset Public.json')\n",
        "\n",
        "print(\"‚úÖ Environment Ready.\")"
      ],
      "metadata": {
        "id": "nI7KWpUtXHmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Loading & Processing"
      ],
      "metadata": {
        "id": "UK8m6X_UXZX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 3: DATA PIPELINE\n",
        "# Purpose: Load and preprocess VQA-RAD images and text.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 1. INITIALIZE PROCESSOR FIRST ---\n",
        "# We need this to exist before creating the dataset!\n",
        "print(\"‚è≥ Initializing Processor...\")\n",
        "# Using the same model ID you defined in your config\n",
        "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
        "\n",
        "# 1. Define Dataset Class (Modified to accept LIST of data)\n",
        "class LlavaRADDataset(Dataset):\n",
        "    def __init__(self, data_list, img_dir, processor, max_length=1024):\n",
        "        self.data = data_list  # Now accepts the split list directly\n",
        "        self.img_dir = img_dir\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        # Load Image\n",
        "        img_path = os.path.join(self.img_dir, item['image_name'])\n",
        "        if not img_path.endswith('.jpg'): img_path += '.jpg'\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Prepare Text\n",
        "        question = item['question']\n",
        "        answer = str(item['answer'])\n",
        "\n",
        "        # LLaVA 1.5 Prompt Format\n",
        "        text_prompt = f\"USER: <image>\\n{question}\\nASSISTANT: {answer}\"\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = self.processor(\n",
        "            text=text_prompt,\n",
        "            images=image,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length\n",
        "        )\n",
        "\n",
        "        input_ids = inputs.input_ids[0]\n",
        "        attention_mask = inputs.attention_mask[0]\n",
        "        pixel_values = inputs.pixel_values[0]\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "# 2. LOAD & SPLIT DATA\n",
        "print(\"‚è≥ Loading and Splitting Data...\")\n",
        "with open(JSON_FILE, 'r') as f:\n",
        "    full_data = json.load(f)\n",
        "\n",
        "# SPLIT: 80% Train / 20% Test (Random State 42 matches Baseline)\n",
        "train_data, test_data = train_test_split(full_data, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"‚úÖ Data Split Complete:\")\n",
        "print(f\"   - Training Samples: {len(train_data)}\")\n",
        "print(f\"   - Test Samples:     {len(test_data)}\")\n",
        "\n",
        "# 3. Create Datasets\n",
        "train_dataset = LlavaRADDataset(train_data, IMAGE_DIR, processor)\n",
        "eval_dataset = LlavaRADDataset(test_data, IMAGE_DIR, processor)"
      ],
      "metadata": {
        "id": "QjxFwknvXZGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exp 5 - The Final Refined version\n",
        "Note: Training was completed previously. The code below is provided for reference/reproducibility."
      ],
      "metadata": {
        "id": "umUP3N1BC-ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 4: THE FINAL SHOWDOWN (Exp 5 - Combined Best Features)\n",
        "# ==============================================================================\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import gc\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig,\n",
        "    LlavaForConditionalGeneration,\n",
        "    TrainerCallback\n",
        ")\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# 1. Clean Slate\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# 2. Setup Live Logging\n",
        "class LiveLoggingCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if state.is_local_process_zero:\n",
        "            log_history = pd.DataFrame(state.log_history)\n",
        "            log_history.to_csv(os.path.join(args.output_dir, \"training_log_live.csv\"), index=False)\n",
        "\n",
        "# 3. Load Model (Skip-Quant Mode)\n",
        "print(\"‚è≥ Loading Base LLaVA Model...\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_skip_modules=[\"multi_modal_projector\"]\n",
        ")\n",
        "\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    \"llava-hf/llava-1.5-7b-hf\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "\n",
        "# --- 3a. PREPARE FOR TRAINING ---\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# 4. CONFIGURATION\n",
        "EXP_NAME = \"Exp5_Final_Validation\"\n",
        "OUTPUT_DIR = f\"{PROJECT_PATH}/results/{EXP_NAME}\"\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=128,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    modules_to_save=[\"multi_modal_projector\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model.enable_input_require_grads()\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# --- 5. MEMORY SAVER FUNCTIONS (CRITICAL FIX) ---\n",
        "# Instead of storing huge logits, we convert to simple integers immediately\n",
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]\n",
        "    return logits.argmax(dim=-1)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    # Now 'predictions' are already small integers, not huge floats\n",
        "    predictions, labels = eval_pred\n",
        "    mask = labels != -100\n",
        "    correct = (predictions[mask] == labels[mask]).sum()\n",
        "    total = mask.sum()\n",
        "    return {\"accuracy\": correct / total}\n",
        "\n",
        "# 6. TRAINING ARGUMENTS\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=2, # <--- Reduced to prevent OOM during Eval\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=30,\n",
        "    learning_rate=1e-4,\n",
        "\n",
        "    # STABILITY SETTINGS\n",
        "    bf16=True,\n",
        "    fp16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        "    dataloader_num_workers=4,\n",
        "    dataloader_pin_memory=True\n",
        ")\n",
        "\n",
        "# 7. TRAINER\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    callbacks=[LiveLoggingCallback()],\n",
        "    compute_metrics=compute_metrics,\n",
        "    # *** THE FIX: Plug in the memory saver ***\n",
        "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
        "    data_collator=lambda x: {\n",
        "        \"input_ids\": torch.stack([i[\"input_ids\"] for i in x]),\n",
        "        \"attention_mask\": torch.stack([i[\"attention_mask\"] for i in x]),\n",
        "        \"pixel_values\": torch.stack([i[\"pixel_values\"] for i in x]),\n",
        "        \"labels\": torch.stack([i[\"labels\"] for i in x])\n",
        "    }\n",
        ")\n",
        "\n",
        "# 8. RUN\n",
        "last_checkpoint = get_last_checkpoint(OUTPUT_DIR)\n",
        "if last_checkpoint:\n",
        "    print(f\"üîÑ Resuming from {last_checkpoint}\")\n",
        "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
        "else:\n",
        "    print(\"üöÄ Starting Exp 5: The Final Validation (Memory Safe Mode)...\")\n",
        "    trainer.train()\n",
        "\n",
        "# 9. SAVE\n",
        "final_path = f\"{OUTPUT_DIR}/final_best_adapter\"\n",
        "trainer.save_model(final_path)\n",
        "print(f\"‚úÖ Final Model Saved to: {final_path}\")"
      ],
      "metadata": {
        "id": "Vx8G6ba7DDU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Final Showdown Scores\n"
      ],
      "metadata": {
        "id": "poxCRYfrIniE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 5: FINAL EVALUATION (With BLEU, ROUGE & BERTScore)\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "# --- 1. RESTORE DATA (Critical Step after Restart) ---\n",
        "print(\"üîÑ Restoring Test Data Split...\")\n",
        "with open(JSON_FILE, 'r') as f:\n",
        "    full_data = json.load(f)\n",
        "\n",
        "# Consistency: Use same random state (42) as Section 3\n",
        "_, test_data = train_test_split(full_data, test_size=0.2, random_state=42)\n",
        "print(f\"‚úÖ Data Restored. Test Set Size: {len(test_data)} images.\")\n",
        "\n",
        "# --- 2. SETUP MODEL & METRICS ---\n",
        "EXP_NAME = \"Exp5_Final_Validation\"\n",
        "ADAPTER_PATH = f\"{PROJECT_PATH}/results/{EXP_NAME}/final_best_adapter\"\n",
        "\n",
        "print(f\"‚è≥ Loading Metrics (BLEU, ROUGE, BERTScore)...\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "print(f\"‚è≥ Loading Model from: {ADAPTER_PATH}\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    llm_int8_skip_modules=[\"multi_modal_projector\"]\n",
        ")\n",
        "\n",
        "base_model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    \"llava-hf/llava-1.5-7b-hf\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "\n",
        "# Merge Adapter\n",
        "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
        "model.eval()\n",
        "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
        "\n",
        "# --- 3. INFERENCE LOOP ---\n",
        "print(\"üöÄ Starting Inference on TEST SET...\")\n",
        "results = []\n",
        "pred_texts = []\n",
        "ref_texts = []\n",
        "\n",
        "for i in tqdm(range(len(test_data))):\n",
        "    item = test_data[i]\n",
        "\n",
        "    # A. Load Image\n",
        "    img_path = os.path.join(IMAGE_DIR, item['image_name'])\n",
        "    if not os.path.exists(img_path) and not img_path.endswith('.jpg'):\n",
        "        img_path += '.jpg'\n",
        "\n",
        "    try:\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "    except:\n",
        "        continue # Skip if image missing\n",
        "\n",
        "    # B. Prepare Text\n",
        "    question = item['question']\n",
        "    ground_truth = str(item['answer']).strip().lower()\n",
        "\n",
        "    # C. Generate\n",
        "    prompt = f\"USER: <image>\\n{question}\\nASSISTANT:\"\n",
        "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        generate_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=40, # Allow enough space for descriptive answers\n",
        "            do_sample=False,\n",
        "            temperature=0.0\n",
        "        )\n",
        "\n",
        "    # D. Decode\n",
        "    generated_text = processor.batch_decode(generate_ids, skip_special_tokens=True)[0]\n",
        "    answer = generated_text.split(\"ASSISTANT:\")[-1].strip().lower()\n",
        "\n",
        "    # --- 4. SCORING LOGIC (The Fix) ---\n",
        "    is_correct = 0\n",
        "\n",
        "    # Logic 1: Exact Match\n",
        "    if answer == ground_truth:\n",
        "        is_correct = 1\n",
        "\n",
        "    # Logic 2: Soft Match (Crucial for Generative Models)\n",
        "    # If the ground truth is \"yes\" or \"no\", allow \"yes, it is...\"\n",
        "    elif ground_truth in [\"yes\", \"no\"]:\n",
        "        # Clean punctuation to ensure \"yes.\" matches \"yes\"\n",
        "        clean_ans = answer.replace('.', '').replace(',', '').strip()\n",
        "        if clean_ans.startswith(ground_truth):\n",
        "            is_correct = 1\n",
        "\n",
        "    # Store Data\n",
        "    pred_texts.append(answer)\n",
        "    ref_texts.append(ground_truth)\n",
        "\n",
        "    # Identify Type based on ANSWER (since JSON tags are unreliable)\n",
        "    q_category = \"CLOSED\" if ground_truth in [\"yes\", \"no\"] else \"OPEN\"\n",
        "\n",
        "    results.append({\n",
        "        \"Question\": question,\n",
        "        \"Category\": q_category,\n",
        "        \"Ground_Truth\": ground_truth,\n",
        "        \"Prediction\": answer,\n",
        "        \"Correct\": is_correct\n",
        "    })\n",
        "\n",
        "# --- 5. CALCULATE & PRINT REPORT ---\n",
        "print(\"\\nüìä Calculating Final Scores...\")\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Accuracy\n",
        "acc_overall = df[\"Correct\"].mean() * 100\n",
        "acc_closed = df[df[\"Category\"] == \"CLOSED\"][\"Correct\"].mean() * 100\n",
        "acc_open = df[df[\"Category\"] == \"OPEN\"][\"Correct\"].mean() * 100\n",
        "\n",
        "# Text Metrics\n",
        "bleu_score = bleu.compute(predictions=pred_texts, references=[[r] for r in ref_texts])\n",
        "rouge_score = rouge.compute(predictions=pred_texts, references=ref_texts)\n",
        "# BERTScore (using distilbert for speed)\n",
        "bert_score = bertscore.compute(predictions=pred_texts, references=ref_texts, lang=\"en\", verbose=False)\n",
        "bert_f1 = np.mean(bert_score['f1'])\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üèÜ EXPERIMENT 5: FINAL SCIENTIFIC REPORT\")\n",
        "print(\"=\"*50)\n",
        "print(f\"1. Clinical Accuracy (Soft-Match):\")\n",
        "print(f\"   - Overall Accuracy:      {acc_overall:.2f}%\")\n",
        "print(f\"   - Closed (Yes/No) Acc:   {acc_closed:.2f}%  <-- USE FOR TABLE 3.3\")\n",
        "print(f\"   - Open (Descriptive) Acc:{acc_open:.2f}%\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"2. Semantic Understanding:\")\n",
        "print(f\"   - BERTScore F1: {bert_f1:.4f} (Target > 0.60)\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"3. Text Structure:\")\n",
        "print(f\"   - BLEU Score:  {bleu_score['bleu']:.4f}\")\n",
        "print(f\"   - ROUGE-L:     {rouge_score['rougeL']:.4f}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Save Final CSV\n",
        "output_csv = f\"{PROJECT_PATH}/results/Exp5_Final_Results_Corrected.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "print(f\"‚úÖ Full results saved to: {output_csv}\")"
      ],
      "metadata": {
        "id": "oGlr7SPrItwl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}