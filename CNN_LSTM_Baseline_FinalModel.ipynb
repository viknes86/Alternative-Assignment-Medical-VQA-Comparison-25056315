{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUDu0O8PAgwg3gOyvh/2wc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viknes86/Alternative-Assignment-Medical-VQA-Comparison-25056315/blob/main/CNN_LSTM_Baseline_FinalModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Model: CNN-LSTM for Medical VQA\n",
        "## Advanced Machine Learning - Final Project\n",
        "**Student Names:** J.Vikneswaran A/L Palaniandy\n",
        "**Student ID:** 25056315\n",
        "\n",
        "### Project Objective\n",
        "To establish a discriminative baseline for the VQA-RAD dataset using a classic **CNN-LSTM** architecture.\n",
        "* **Visual Encoder:** ResNet50 (Pretrained on ImageNet) to extract visual features.\n",
        "* **Question Encoder:** LSTM (Long Short-Term Memory) to process text.\n",
        "* **Fusion:** Element-wise multiplication of visual and textual features.\n",
        "* **Classifier:** Fully Connected Layer predicting one word from a fixed vocabulary.\n",
        "\n",
        "This baseline will be compared against the generative **LLaVA-Med** model to demonstrate the \"Capacity Wall\" of discriminative approaches in medical reasoning."
      ],
      "metadata": {
        "id": "FSBYcxxjevb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Drive & Setup"
      ],
      "metadata": {
        "id": "P-AHWwzQe9bk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CVqRmUweWlZ"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# SECTION 1: ENVIRONMENT & SETUP\n",
        "# Purpose: Mount Drive and define project paths.\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define Paths\n",
        "# UPDATE THIS if your folder name is different\n",
        "PROJECT_PATH = '/content/drive/MyDrive/AML_FinalProject'\n",
        "IMAGE_DIR = os.path.join(PROJECT_PATH, 'VQA_RAD Image Folder')\n",
        "JSON_FILE = os.path.join(PROJECT_PATH, 'VQA_RAD Dataset Public.json')\n",
        "MODEL_SAVE_PATH = os.path.join(PROJECT_PATH, 'cnn_lstm_vqa.pth')\n",
        "\n",
        "# 3. Device Config\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"âœ… Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vocabulary & Dataset"
      ],
      "metadata": {
        "id": "27gafI20fMQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 2: DATA PIPELINE\n",
        "# Purpose: Build Vocabulary and Custom Dataset Class.\n",
        "# ==============================================================================\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.idx2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.idx = 4\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        return self.word2idx.get(word, self.word2idx[\"<UNK>\"])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "def build_vocab(json_path, threshold=1):\n",
        "    \"\"\"Builds vocabulary from the VQA-RAD questions and answers.\"\"\"\n",
        "    df = pd.read_json(json_path)\n",
        "    counter = Counter()\n",
        "\n",
        "    # Tokenize Questions & Answers\n",
        "    for question in df['question']:\n",
        "        tokens = nltk.tokenize.word_tokenize(str(question).lower())\n",
        "        counter.update(tokens)\n",
        "\n",
        "    # Add Answers to Vocab (Critical for classification)\n",
        "    for answer in df['answer']:\n",
        "        tokens = str(answer).lower().split() # Simple split for answers\n",
        "        counter.update(tokens)\n",
        "\n",
        "    # Create Vocab Object\n",
        "    vocab = Vocabulary()\n",
        "    for word, count in counter.items():\n",
        "        if count >= threshold:\n",
        "            vocab.add_word(word)\n",
        "\n",
        "    print(f\"âœ… Vocabulary Built. Total Size: {len(vocab)}\")\n",
        "    return vocab\n",
        "\n",
        "# Custom Dataset\n",
        "class VQARADDataset(Dataset):\n",
        "    def __init__(self, json_file, img_dir, vocab, transform=None):\n",
        "        self.data = pd.read_json(json_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "\n",
        "        # 1. Load Image\n",
        "        img_name = item['image_name']\n",
        "        if not img_name.endswith('.jpg'): img_name += '.jpg'\n",
        "        image = Image.open(os.path.join(self.img_dir, img_name)).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # 2. Process Text (Question)\n",
        "        tokens = nltk.tokenize.word_tokenize(str(item['question']).lower())\n",
        "        q_indices = [self.vocab(\"<SOS>\")] + [self.vocab(token) for token in tokens] + [self.vocab(\"<EOS>\")]\n",
        "\n",
        "        # Pad/Truncate to fixed length (e.g., 20)\n",
        "        max_len = 20\n",
        "        if len(q_indices) < max_len:\n",
        "            q_indices += [self.vocab(\"<PAD>\")] * (max_len - len(q_indices))\n",
        "        else:\n",
        "            q_indices = q_indices[:max_len]\n",
        "\n",
        "        # 3. Process Label (Answer)\n",
        "        # For CNN-LSTM, we treat VQA as classification over the Vocab\n",
        "        ans_token = str(item['answer']).lower().split()[0] # Take first word as label\n",
        "        label = self.vocab(ans_token)\n",
        "\n",
        "        return image, torch.tensor(q_indices), torch.tensor(label)\n",
        "\n",
        "# Setup Transforms (Standard ResNet Norms)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "nltk.download('punkt')\n",
        "vocab = build_vocab(JSON_FILE)\n",
        "dataset = VQARADDataset(JSON_FILE, IMAGE_DIR, vocab, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "print(\"âœ… Dataset Ready.\")"
      ],
      "metadata": {
        "id": "NApKMgE6fNBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Architecture"
      ],
      "metadata": {
        "id": "jmcSmEehfQSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 3: MODEL ARCHITECTURE (CNN-LSTM)\n",
        "# Purpose: Define the Visual Encoder and Text Decoder.\n",
        "# ==============================================================================\n",
        "\n",
        "class CNN_LSTM_VQA(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=128, hidden_size=256, num_layers=1):\n",
        "        super(CNN_LSTM_VQA, self).__init__()\n",
        "\n",
        "        # 1. Visual Encoder (ResNet50)\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        modules = list(resnet.children())[:-1] # Remove FC layer\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.visual_fc = nn.Linear(2048, hidden_size) # Project to LSTM dimension\n",
        "\n",
        "        # 2. Question Encoder (LSTM)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # 3. Classifier\n",
        "        self.classifier = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, images, questions):\n",
        "        # A. Image Features\n",
        "        features = self.resnet(images)\n",
        "        features = features.view(features.size(0), -1) # Flatten (Batch, 2048)\n",
        "        img_embedding = self.visual_fc(features)       # (Batch, 256)\n",
        "\n",
        "        # B. Text Features\n",
        "        embeds = self.embedding(questions)             # (Batch, Seq, 128)\n",
        "        _, (hidden, _) = self.lstm(embeds)             # Get final hidden state\n",
        "        txt_embedding = hidden[-1]                     # (Batch, 256)\n",
        "\n",
        "        # C. Fusion (Element-wise Multiplication)\n",
        "        fused = img_embedding * txt_embedding\n",
        "\n",
        "        # D. Prediction\n",
        "        logits = self.classifier(fused)\n",
        "        return logits\n",
        "\n",
        "# Initialize Model\n",
        "model = CNN_LSTM_VQA(vocab_size=len(vocab)).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "djVms9DkfRtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "2XGbH3btfVkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 4: TRAINING (Run if retraining is needed)\n",
        "# ==============================================================================\n",
        "\n",
        "def train_model(model, loader, epochs=20):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for imgs, qs, labels in loader:\n",
        "            imgs, qs, labels = imgs.to(device), qs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs, qs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(loader)\n",
        "        loss_history.append(avg_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save Model\n",
        "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "    print(f\"âœ… Model Saved to {MODEL_SAVE_PATH}\")\n",
        "    return loss_history\n",
        "\n",
        "# Uncomment to train\n",
        "# history = train_model(model, dataloader)"
      ],
      "metadata": {
        "id": "S2x6DvrufZXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation & Visualization"
      ],
      "metadata": {
        "id": "jyCosEzNfbcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 5: EVALUATION & VISUALIZATION\n",
        "# Purpose: Calculate Accuracy and visualize Predictions.\n",
        "# ==============================================================================\n",
        "\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    print(\"ðŸ”Ž Evaluating...\")\n",
        "    with torch.no_grad():\n",
        "        for imgs, qs, labels in loader:\n",
        "            imgs, qs, labels = imgs.to(device), qs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(imgs, qs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    print(f\"âœ… Final Accuracy: {acc:.2f}%\")\n",
        "    return acc\n",
        "\n",
        "# Load Saved Weights (if available)\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "    model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "    print(\"ðŸ“‚ Loaded Pre-trained Weights.\")\n",
        "\n",
        "evaluate_model(model, dataloader)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Visualization: Qualitative Results\n",
        "# ---------------------------------------------------------\n",
        "def visualize_predictions(model, dataset, num_samples=3):\n",
        "    model.eval()\n",
        "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        img, q_tensor, label_tensor = dataset[idx]\n",
        "\n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            output = model(img.unsqueeze(0).to(device), q_tensor.unsqueeze(0).to(device))\n",
        "            pred_idx = output.argmax(1).item()\n",
        "\n",
        "        # Decode\n",
        "        q_text = \" \".join([dataset.vocab.idx2word[t.item()] for t in q_tensor if t.item() not in [0,1,2]])\n",
        "        truth = dataset.vocab.idx2word[label_tensor.item()]\n",
        "        pred = dataset.vocab.idx2word[pred_idx]\n",
        "\n",
        "        # Plot\n",
        "        ax = plt.subplot(1, 3, i+1)\n",
        "        # Denormalize image for display\n",
        "        img_disp = img.permute(1, 2, 0).numpy()\n",
        "        img_disp = img_disp * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "\n",
        "        plt.imshow(np.clip(img_disp, 0, 1))\n",
        "        plt.title(f\"Q: {q_text}\\nTrue: {truth} | Pred: {pred}\", fontsize=10)\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Color coding title\n",
        "        if truth == pred:\n",
        "            ax.set_title(ax.get_title(), color='green', fontweight='bold')\n",
        "        else:\n",
        "            ax.set_title(ax.get_title(), color='red')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_predictions(model, dataset)"
      ],
      "metadata": {
        "id": "dHqC2bjDfdg7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}