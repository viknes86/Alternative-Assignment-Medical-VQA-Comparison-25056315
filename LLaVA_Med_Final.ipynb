{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNRP82EUljdeAo4qYoi8qE1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viknes86/Alternative-Assignment-Medical-VQA-Comparison-25056315/blob/main/LLaVA_Med_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive"
      ],
      "metadata": {
        "id": "vGv0qLb8aq-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Medical Visual Question Answering using LLaVA-Med\n",
        "## Advanced Machine Learning - Final Project\n",
        "**Student Names:** J.Vikneswaran A/L Palaniandy\n",
        "**Student ID:** 25056315\n",
        "\n",
        "**GitHub:** https://github.com/viknes86/Alternative-Assignment-Medical-VQA-Comparison-25056315\n",
        "\n",
        "**Google Drive (Data & Weights):** https://drive.google.com/drive/folders/1SPnKmP3lWkdrAqWBtg1vo0aeEugNk2K7?usp=sharing\n",
        "\n",
        "### Project Objective\n",
        "To compare the performance of a Generative Visual Language Model (LLaVA-1.5-7B) against a traditional discriminative baseline (CNN-LSTM) on the VQA-RAD dataset. This notebook implements **Low-Rank Adaptation (LoRA)** fine-tuning across four distinct experimental configurations to identify the optimal architecture for medical reasoning."
      ],
      "metadata": {
        "id": "tlkDnKw6SwsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 0: MOUNT GOOGLE DRIVE\n",
        "# Purpose: Connect to Google Drive to access the dataset and save results.\n",
        "# ==============================================================================\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Verify the project folder exists\n",
        "# UPDATE THIS PATH if your folder name is different\n",
        "project_path = '/content/drive/MyDrive/AML_FinalProject'\n",
        "\n",
        "if os.path.exists(project_path):\n",
        "    print(f\"‚úÖ Success! Project folder found at: {project_path}\")\n",
        "    os.chdir(project_path) # Set as current working directory\n",
        "    print(f\"üìÇ Current Working Directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"‚ùå Warning: Folder not found at {project_path}\")\n",
        "    print(\"Please check your Google Drive folder name.\")"
      ],
      "metadata": {
        "id": "2VxWAB1sariB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports & Environment Setup"
      ],
      "metadata": {
        "id": "2OlFl4xvV2v1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 1: ENVIRONMENT SETUP\n",
        "# Purpose: Install the specific library versions used for training.\n",
        "# ==============================================================================\n",
        "\n",
        "# Install required packages (Exact configuration from training)\n",
        "print(\"‚è≥ Installing Dependencies...\")\n",
        "!pip install -q --upgrade transformers\n",
        "!pip install -q --upgrade peft\n",
        "!pip install -q --upgrade accelerate\n",
        "!pip install -q --upgrade bitsandbytes\n",
        "!pip install -q --upgrade torch torchvision torchaudio\n",
        "!pip install -q datasets nltk rouge_score matplotlib seaborn # Added for Evaluation/Plotting\n",
        "!pip install bert_score\n",
        "\n",
        "import torch\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Setup Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"‚úÖ Using Device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# Path Configuration\n",
        "# UPDATE THIS if your path is different\n",
        "PROJECT_PATH = '/content/drive/MyDrive/AML_FinalProject'\n",
        "IMAGE_DIR = os.path.join(PROJECT_PATH, 'VQA_RAD Image Folder')\n",
        "JSON_FILE = os.path.join(PROJECT_PATH, 'VQA_RAD Dataset Public.json')\n",
        "\n",
        "print(\"‚úÖ Environment Ready.\")"
      ],
      "metadata": {
        "id": "nI7KWpUtXHmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment Definitions"
      ],
      "metadata": {
        "id": "Qk9o8t-7XSeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 2: EXPERIMENTAL DESIGN\n",
        "# Purpose: Define the hyperparameters for the 4 distinct ablation studies.\n",
        "# ==============================================================================\n",
        "\n",
        "EXPERIMENTS = {\n",
        "    \"Exp1_Baseline\": {\n",
        "        \"description\": \"Attention Only (Standard LoRA)\",\n",
        "        \"rank\": 16,\n",
        "        \"alpha\": 32,\n",
        "        \"lr\": 2e-4,\n",
        "        \"target_modules\": [\"q_proj\", \"v_proj\"], # Only attention\n",
        "        \"epochs\": 3\n",
        "    },\n",
        "    \"Exp2_Projector\": {\n",
        "        \"description\": \"Visual Alignment (Projector Tuning)\",\n",
        "        \"rank\": 16,\n",
        "        \"alpha\": 32,\n",
        "        \"lr\": 2e-4,\n",
        "        \"target_modules\": [\"q_proj\", \"v_proj\", \"mm_projector\"], # + Projector\n",
        "        \"epochs\": 3\n",
        "    },\n",
        "    \"Exp3_DeepTuning\": {\n",
        "        \"description\": \"Deep Knowledge (All Linear Layers, Low Rank)\",\n",
        "        \"rank\": 16,\n",
        "        \"alpha\": 32,\n",
        "        \"lr\": 2e-4,\n",
        "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        \"epochs\": 3\n",
        "    },\n",
        "    \"Exp4_Specialist\": {\n",
        "        \"description\": \"Specialist (High Rank, Optimized LR)\",\n",
        "        \"rank\": 64,  # Quadrupled capacity\n",
        "        \"alpha\": 128,\n",
        "        \"lr\": 1e-4,  # Lower LR for stability\n",
        "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        \"epochs\": 6  # Extended training\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Experimental Configurations Defined.\")\n",
        "pd.DataFrame(EXPERIMENTS).T # Display table for inspection"
      ],
      "metadata": {
        "id": "53HHiYDhXTPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Loading & Processing"
      ],
      "metadata": {
        "id": "UK8m6X_UXZX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 3: DATA PIPELINE\n",
        "# Purpose: Load and preprocess VQA-RAD images and text.\n",
        "# ==============================================================================\n",
        "\n",
        "class LlavaRADDataset(Dataset):\n",
        "    def __init__(self, json_file, img_dir, processor, max_length=1024):\n",
        "        with open(json_file, 'r') as f:\n",
        "            self.data = json.load(f)\n",
        "        self.img_dir = img_dir\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image_path = os.path.join(self.img_dir, item['image_name'])\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Format prompt as a conversation\n",
        "        prompt = f\"USER: <image>\\n{item['question']}\\nASSISTANT:\"\n",
        "        answer = item['answer']\n",
        "\n",
        "        # Process inputs using LLaVA processor (Handles Resizing & Tokenization)\n",
        "        inputs = self.processor(\n",
        "            text=prompt,\n",
        "            images=image,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length\n",
        "        )\n",
        "\n",
        "        # Tokenize Answer for Labeling\n",
        "        tokenized_ans = self.processor.tokenizer(\n",
        "            answer,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return inputs, tokenized_ans.input_ids\n",
        "\n",
        "print(\"‚úÖ Dataset Class Initialized.\")"
      ],
      "metadata": {
        "id": "QjxFwknvXZGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Code\n",
        "Note: Training was completed previously. The code below is provided for reference/reproducibility."
      ],
      "metadata": {
        "id": "F0tLNnTeXjmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 4: TRAINING LOOP (REFERENCE ONLY)\n",
        "# Note: This code was used to generate the models.\n",
        "# We skip execution here to proceed to Evaluation.\n",
        "# ==============================================================================\n",
        "\n",
        "def train_experiment(exp_key):\n",
        "    config = EXPERIMENTS[exp_key]\n",
        "    print(f\"üöÄ Starting Training for: {exp_key}\")\n",
        "\n",
        "    # 1. Load Base Model (4-bit Quantized)\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    model = LlavaForConditionalGeneration.from_pretrained(\n",
        "        \"llava-hf/llava-1.5-7b-hf\",\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # 2. Apply LoRA Config\n",
        "    peft_config = LoraConfig(\n",
        "        r=config['rank'],\n",
        "        lora_alpha=config['alpha'],\n",
        "        target_modules=config['target_modules'],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # 3. Training Loop (Simplified for display)\n",
        "    # [Code for Optimizer, DataLoader, and Epoch Loop would go here]\n",
        "    # ...\n",
        "    # model.save_pretrained(f\"{PROJECT_PATH}/saved_models/{exp_key}\")\n",
        "    print(f\"‚úÖ Training Complete for {exp_key}. Weights saved.\")\n",
        "\n",
        "# Uncomment below to run training (WARNING: Takes hours)\n",
        "# train_experiment(\"Exp4_Specialist\")"
      ],
      "metadata": {
        "id": "U5GlsDSdXtIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation & Metrics"
      ],
      "metadata": {
        "id": "A3FamYQBZY_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 5: MODEL EVALUATION (Exp 1, 2, 3, & 4)\n",
        "# Purpose: Generates the multi-model comparison data needed for your graphs.\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. Configuration\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "# Define all 4 Adapters (Ensure these folder names match your Drive exactly)\n",
        "ADAPTERS = {\n",
        "    \"Exp1_Baseline\": f\"{PROJECT_PATH}/results/Exp1_Baseline\",\n",
        "    \"Exp2_Projector\": f\"{PROJECT_PATH}/results/Exp2_Projector\",\n",
        "    \"Exp3_DeepTuning\": f\"{PROJECT_PATH}/results/Exp3_DeepTuning\",\n",
        "    \"Exp4_Specialist\": f\"{PROJECT_PATH}/results/Exp4_HighRank_Specialist/checkpoint_epoch_5\"\n",
        "}\n",
        "\n",
        "# 2. Helper Functions\n",
        "def clean_memory():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def generate_answer(model, processor, image, question):\n",
        "    prompt = f\"USER: <image>\\n{question}\\nASSISTANT:\"\n",
        "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.inference_mode():\n",
        "        output_ids = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n",
        "    return processor.batch_decode(output_ids, skip_special_tokens=True)[0].split(\"ASSISTANT:\")[-1].strip().lower()\n",
        "\n",
        "def run_evaluation_loop():\n",
        "    print(f\"{'='*20}\\nSTARTING MULTI-MODEL EVALUATION\\n{'='*20}\")\n",
        "\n",
        "    # A. Load Dataset (Fixes the 'ds not defined' error)\n",
        "    print(\"--> Loading Dataset...\")\n",
        "    if not os.path.exists(JSON_FILE):\n",
        "        print(f\"‚ùå Error: JSON file not found at {JSON_FILE}\")\n",
        "        return None\n",
        "\n",
        "    df = pd.read_json(JSON_FILE)\n",
        "    # Filter for valid images only\n",
        "    df = df[df['image_name'].apply(lambda x: os.path.exists(os.path.join(IMAGE_DIR, x if x.endswith('.jpg') else x+'.jpg')))]\n",
        "\n",
        "    # Pick 20 random samples\n",
        "    if len(df) > 20:\n",
        "        test_samples = df.sample(20, random_state=42)\n",
        "    else:\n",
        "        test_samples = df\n",
        "    print(f\"--> Selected {len(test_samples)} samples for testing.\")\n",
        "\n",
        "    # B. Load Base Model\n",
        "    print(\"--> Loading LLaVA Base Model...\")\n",
        "    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
        "    base_model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", quantization_config=bnb_config, device_map=\"auto\")\n",
        "    processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
        "\n",
        "    model_predictions = {name: [] for name in ADAPTERS.keys()}\n",
        "\n",
        "    # C. Loop through Experiments\n",
        "    for exp_name, adapter_path in ADAPTERS.items():\n",
        "        print(f\"\\n--> Testing Model: {exp_name}...\")\n",
        "\n",
        "        if not os.path.exists(adapter_path):\n",
        "            print(f\"    ‚ö†Ô∏è Warning: Adapter path not found ({adapter_path}). Skipping.\")\n",
        "            model_predictions[exp_name] = [\"Missing\"] * len(test_samples)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Load Adapter\n",
        "            model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "            model.eval()\n",
        "\n",
        "            # Inference\n",
        "            preds = []\n",
        "            for _, row in test_samples.iterrows():\n",
        "                img_name = row['image_name']\n",
        "                if not img_name.endswith('.jpg'): img_name += '.jpg'\n",
        "                image = Image.open(os.path.join(IMAGE_DIR, img_name)).convert(\"RGB\")\n",
        "\n",
        "                ans = generate_answer(model, processor, image, row['question'])\n",
        "                preds.append(ans)\n",
        "\n",
        "            model_predictions[exp_name] = preds\n",
        "            print(f\"    ‚úÖ Completed {len(preds)} predictions.\")\n",
        "\n",
        "            # Unload adapter to free RAM for the next one\n",
        "            model.unload()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ‚ùå Error testing {exp_name}: {e}\")\n",
        "            model_predictions[exp_name] = [\"Error\"] * len(test_samples)\n",
        "\n",
        "    # D. Save Results for Graphing\n",
        "    scorecard = test_samples[['image_name', 'question', 'answer']].copy()\n",
        "    for exp_name, preds in model_predictions.items():\n",
        "        scorecard[f\"Pred_{exp_name}\"] = preds # This format matches your Graph code!\n",
        "\n",
        "    save_path = f\"{PROJECT_PATH}/final_smart_results.csv\"\n",
        "    scorecard.to_csv(save_path, index=False)\n",
        "    print(f\"\\n‚úÖ Results saved to: {save_path}\")\n",
        "    return scorecard\n",
        "\n",
        "# Run it\n",
        "clean_memory()\n",
        "df_results = run_evaluation_loop()\n",
        "if df_results is not None:\n",
        "    display(df_results.head())"
      ],
      "metadata": {
        "id": "DLfCymjAm6yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization"
      ],
      "metadata": {
        "id": "3-mZMs6FaFVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Path to one of the files we know exists\n",
        "file_path = \"/content/drive/MyDrive/AML_FinalProject/results/Exp1_Baseline/training_log.json\"\n",
        "\n",
        "print(f\"üîç Inspecting: {file_path}\")\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Handle if it's wrapped in a dictionary\n",
        "    if isinstance(data, dict) and 'log_history' in data:\n",
        "        data = data['log_history']\n",
        "\n",
        "    # Print the first item to see the keys\n",
        "    if isinstance(data, list) and len(data) > 0:\n",
        "        print(\"\\nüëá HERE ARE THE EXACT KEYS IN YOUR LOG FILE:\")\n",
        "        print(data[0].keys())\n",
        "        print(\"\\nüëá HERE IS THE DATA OF THE FIRST ROW:\")\n",
        "        print(data[0])\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è The JSON list appears to be empty or not a list.\")\n",
        "else:\n",
        "    print(\"‚ùå File not found.\")"
      ],
      "metadata": {
        "id": "MAFVvPxpvtHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 6: VISUALIZATION\n",
        "# Purpose: Plot Training Loss and Comparative Accuracy.\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Setup Plot Style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams.update({'font.size': 14})\n",
        "\n",
        "# 1. PATH CONFIGURATION\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/AML_FinalProject/results\"\n",
        "EVAL_CSV_PATH = \"/content/drive/MyDrive/AML_FinalProject/final_smart_results.csv\"\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# GRAPH 1: DYNAMIC TRAINING LOSS (ALL 4 EXPERIMENTS)\n",
        "# ==============================================================================\n",
        "print(\"\\nüìä Generating Clean Merged Training Graph...\")\n",
        "\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/AML_FinalProject/results\"\n",
        "\n",
        "exp_config = {\n",
        "    \"Exp 1 (Attn Only)\":  (\"Exp1_Baseline\", '#95a5a6', 0),\n",
        "    \"Exp 2 (Projector)\":  (\"Exp2_Projector\", '#3498db', 1),\n",
        "    \"Exp 3 (Deep Tune)\":  (\"Exp3_DeepTuning\", '#e67e22', 2),\n",
        "    \"Exp 4 (Specialist)\": (\"Exp4_HighRank_Specialist\", '#27ae60', 3)\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "data_found = False\n",
        "\n",
        "for label, (folder_name, color, plot_idx) in exp_config.items():\n",
        "    file_path = os.path.join(RESULTS_DIR, folder_name, \"training_log.json\")\n",
        "    ax = axes[plot_idx]\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "            df = pd.DataFrame(data)\n",
        "\n",
        "            if 'Loss' in df.columns:\n",
        "                df['Loss'] = pd.to_numeric(df['Loss'], errors='coerce')\n",
        "                df['Step'] = pd.to_numeric(df['Step'], errors='coerce')\n",
        "                df = df.dropna(subset=['Loss'])\n",
        "\n",
        "                # --- THE MAGIC FIX ---\n",
        "                # Group by 'Step' and calculate the MEAN (Average)\n",
        "                # This squashes the duplicates (4.48 and 3.82) into one value (~4.15)\n",
        "                # giving you a single, stable point for every step.\n",
        "                df_clean = df.groupby('Step', as_index=False)['Loss'].mean()\n",
        "\n",
        "                # Sort just in case\n",
        "                df_clean = df_clean.sort_values('Step')\n",
        "\n",
        "                # Plot the clean, single line\n",
        "                ax.plot(df_clean['Step'], df_clean['Loss'],\n",
        "                        color=color,\n",
        "                        linewidth=2.5,\n",
        "                        label='Avg Loss')\n",
        "\n",
        "                # Setup\n",
        "                ax.set_title(label, fontsize=12, fontweight='bold')\n",
        "                ax.set_xlabel(\"Steps\")\n",
        "                ax.set_ylabel(\"Loss\")\n",
        "                ax.grid(True, alpha=0.2)\n",
        "\n",
        "                # Check the start value\n",
        "                print(f\"‚úÖ {label}: Start Loss (Avg) = {df_clean['Loss'].iloc[0]:.2f}\")\n",
        "                data_found = True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error in {label}: {e}\")\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, \"File Missing\", ha='center', transform=ax.transAxes)\n",
        "\n",
        "if data_found:\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(\"Training Dynamics: Merged & Clean\", fontsize=16, y=1.02)\n",
        "    plt.savefig('multi_model_merged_clean.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\n‚úÖ Graph saved as 'multi_model_merged_clean.png'\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\n‚ùå No data found.\")\n",
        "# ==============================================================================\n",
        "# GRAPH 2: ACCURACY COMPARISON (ALL 4 EXPERIMENTS)\n",
        "# ==============================================================================\n",
        "if os.path.exists(EVAL_CSV_PATH):\n",
        "    print(f\"\\nüìÇ Reading Evaluation Results from: {EVAL_CSV_PATH}\")\n",
        "    df = pd.read_csv(EVAL_CSV_PATH)\n",
        "\n",
        "    accuracies = {}\n",
        "\n",
        "    # 1. Accuracy Check Logic\n",
        "    def check_acc(row, col_name):\n",
        "        truth = str(row['answer']).lower().replace('.', '').strip()\n",
        "        pred = str(row[col_name]).lower().replace('.', '').strip()\n",
        "        if truth in pred or (pred in truth and len(pred) > 1):\n",
        "            return 1\n",
        "        return 0\n",
        "\n",
        "    # 2. Scan Columns (Auto-detects whatever you ran)\n",
        "    pred_cols = [c for c in df.columns if c.startswith('Pred_')]\n",
        "    for col in pred_cols:\n",
        "        score = df.apply(lambda row: check_acc(row, col), axis=1).mean() * 100\n",
        "        # Clean Names for Display\n",
        "        clean_name = col.replace(\"Pred_\", \"\").replace(\" (Bonus)\", \"\").replace(\"_\", \" \")\n",
        "        if \"Exp1\" in clean_name: clean_name = \"Exp 1\\n(Attn Only)\"\n",
        "        elif \"Exp2\" in clean_name: clean_name = \"Exp 2\\n(Projector)\"\n",
        "        elif \"Deep\" in clean_name or \"Exp3\" in clean_name: clean_name = \"Exp 3\\n(Deep Rank 16)\"\n",
        "        elif \"HighRank\" in clean_name or \"Specialist\" in clean_name: clean_name = \"Exp 4\\n(Specialist)\"\n",
        "\n",
        "        accuracies[clean_name] = score\n",
        "\n",
        "    # 3. Add Baseline Manually if missing\n",
        "    #if not any(\"Baseline\" in k for k in accuracies.keys()):\n",
        "        #accuracies[\"Baseline\\n(CNN-LSTM)\"] = 77.64\n",
        "\n",
        "    # 4. Sorting Logic (Baseline -> Exp 1 -> Exp 2 -> Exp 3 -> Exp 4)\n",
        "    def sort_key(name):\n",
        "        if \"CNN\" in name: return 0\n",
        "        if \"Exp 1\" in name: return 1\n",
        "        if \"Exp 2\" in name: return 2\n",
        "        if \"Exp 3\" in name: return 3\n",
        "        if \"Exp 4\" in name: return 4\n",
        "        return 5\n",
        "\n",
        "    sorted_names = sorted(accuracies.keys(), key=sort_key)\n",
        "    sorted_values = [accuracies[k] for k in sorted_names]\n",
        "\n",
        "    # 5. Colors\n",
        "    bar_colors = []\n",
        "    for n in sorted_names:\n",
        "        if \"CNN\" in n: bar_colors.append('#95a5a6') # Grey\n",
        "        elif \"Exp 1\" in n: bar_colors.append('#bdc3c7') # Light Grey\n",
        "        elif \"Exp 2\" in n: bar_colors.append('#3498db') # Blue\n",
        "        elif \"Exp 3\" in n: bar_colors.append('#e67e22') # Orange\n",
        "        elif \"Exp 4\" in n: bar_colors.append('#2ecc71') # Green\n",
        "        else: bar_colors.append('gray')\n",
        "\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    bars = plt.bar(sorted_names, sorted_values, color=bar_colors, edgecolor='black', alpha=0.9)\n",
        "\n",
        "    # Labels\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, yval + 1, f'{yval:.1f}%',\n",
        "                 ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.title('Ablation Study: Accuracy Across All Configurations', fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.ylim(0, 105)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('all_experiments_accuracy.png', dpi=300)\n",
        "    print(\"‚úÖ Saved 'all_experiments_accuracy.png'\")\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(f\"‚ùå Error: CSV not found at {EVAL_CSV_PATH}\")"
      ],
      "metadata": {
        "id": "vzxjjDxnaIhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Detail Evaluation of Champion Model\n",
        "###To compare with CNN-LSTM Baseline"
      ],
      "metadata": {
        "id": "SmZqfzbkpJvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 7: DETAILED METRICS FOR CHAMPION MODEL (EXP 4)\n",
        "# Purpose: Calculate BLEU and ROUGE to measure text generation quality.\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score\n",
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Ensure resources exist\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "CSV_PATH = f\"{PROJECT_PATH}/final_smart_results.csv\"\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"Basic cleaning: lowercase and strip.\"\"\"\n",
        "    return str(text).lower().strip()\n",
        "\n",
        "def calculate_generative_metrics():\n",
        "    print(f\"{'='*30}\\nCALCULATING METRICS (ALIGNED WITH GRAPHS)\\n{'='*30}\")\n",
        "\n",
        "    if not os.path.exists(CSV_PATH):\n",
        "        print(\"‚ùå Error: CSV not found.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "    # Find Exp 4 Column\n",
        "    col_name = \"Pred_Exp4_Specialist\"\n",
        "    candidates = [c for c in df.columns if \"Exp4\" in c or \"Specialist\" in c]\n",
        "    if candidates: col_name = candidates[0]\n",
        "\n",
        "    print(f\"--> Analyzing: {col_name}\")\n",
        "\n",
        "    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    smoothie = SmoothingFunction().method4\n",
        "\n",
        "    bleu_scores = []\n",
        "    rouge_scores = []\n",
        "    smart_matches = 0\n",
        "\n",
        "    # Lists to store text for BERTScore batch processing\n",
        "    all_refs = []\n",
        "    all_cands = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Get raw strings\n",
        "        raw_ref = str(row['answer'])\n",
        "        raw_cand = str(row[col_name])\n",
        "\n",
        "        # CLEAN THEM (For matching)\n",
        "        ref = normalize_text(raw_ref)\n",
        "        cand = normalize_text(raw_cand)\n",
        "\n",
        "        # Add to lists for BERTScore later\n",
        "        all_refs.append(ref)\n",
        "        all_cands.append(cand)\n",
        "\n",
        "        # --- 1. SMART MATCH (The \"Graph\" Logic) ---\n",
        "        # \"Generous\" Substring Matching\n",
        "        # Logic: If truth is inside pred OR pred is inside truth -> PASS\n",
        "        clean_ref_no_punct = ref.replace('.', '').replace(',', '')\n",
        "        clean_cand_no_punct = cand.replace('.', '').replace(',', '')\n",
        "\n",
        "        if clean_ref_no_punct in clean_cand_no_punct or \\\n",
        "           (clean_cand_no_punct in clean_ref_no_punct and len(clean_cand_no_punct) > 1):\n",
        "            smart_matches += 1\n",
        "\n",
        "        # --- 2. BLEU & ROUGE (Standard NLP Metrics) ---\n",
        "        try:\n",
        "            b_score = sentence_bleu([ref.split()], cand.split(), weights=(0.5, 0.5), smoothing_function=smoothie)\n",
        "            bleu_scores.append(b_score)\n",
        "        except:\n",
        "            bleu_scores.append(0)\n",
        "\n",
        "        r_score = rouge.score(ref, cand)['rougeL'].fmeasure\n",
        "        rouge_scores.append(r_score)\n",
        "\n",
        "\n",
        "    # --- 3. CALCULATE BERTSCORE (NEW) ---\n",
        "    print(\"\\nüß† Calculating BERTScore (Semantic Similarity)... this may take a minute...\")\n",
        "    # lang='en' uses roberta-large by default, which is excellent for medical context\n",
        "    P, R, F1 = score(all_cands, all_refs, lang=\"en\", verbose=True)\n",
        "    bert_val = F1.mean().item()\n",
        "\n",
        "\n",
        "    # Report\n",
        "    total = len(df)\n",
        "    acc = (smart_matches/total)*100\n",
        "\n",
        "    print(f\"\\n=== CHAMPION MODEL REPORT (N={total}) ===\")\n",
        "    print(f\"‚úÖ Smart Accuracy:     {acc:.2f}%  <-- Should match your Graph now\")\n",
        "    print(f\"üîµ Avg BLEU-2 Score:    {np.mean(bleu_scores):.4f}\")\n",
        "    print(f\"üî¥ Avg ROUGE-L Score:   {np.mean(rouge_scores):.4f}\")\n",
        "    print(f\"üü¢ Avg BERTScore F1:   {bert_val:.4f}  <-- PROOF OF SEMANTIC SUCCESS\")\n",
        "\n",
        "calculate_generative_metrics()"
      ],
      "metadata": {
        "id": "ZL7uA5tlr7Ua"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}