{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNhKT9tf/U/lmYdGVt4DkXc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viknes86/Alternative-Assignment-Medical-VQA-Comparison-25056315/blob/main/01_Baseline_CNN_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Model: CNN-LSTM for Medical VQA\n",
        "## Advanced Machine Learning - Final Project\n",
        "**Student Names:** J.Vikneswaran A/L Palaniandy\n",
        "**Student ID:** 25056315\n",
        "\n",
        "**GitHub:** https://github.com/viknes86/Alternative-Assignment-Medical-VQA-Comparison-25056315\n",
        "\n",
        "**Google Drive (Data & Weights):** https://drive.google.com/drive/folders/1SPnKmP3lWkdrAqWBtg1vo0aeEugNk2K7?usp=sharing\n",
        "\n",
        "\n",
        "### Project Objective\n",
        "To establish a discriminative baseline for the VQA-RAD dataset using a classic **CNN-LSTM** architecture.\n",
        "* **Visual Encoder:** ResNet50 (Pretrained on ImageNet) to extract visual features.\n",
        "* **Question Encoder:** LSTM (Long Short-Term Memory) to process text.\n",
        "* **Fusion:** Element-wise multiplication of visual and textual features.\n",
        "* **Classifier:** Fully Connected Layer predicting one word from a fixed vocabulary.\n",
        "\n",
        "This baseline will be compared against the generative **LLaVA-Med** model to demonstrate the \"Capacity Wall\" of discriminative approaches in medical reasoning."
      ],
      "metadata": {
        "id": "FSBYcxxjevb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Drive & Setup"
      ],
      "metadata": {
        "id": "P-AHWwzQe9bk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CVqRmUweWlZ"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# SECTION 1: ENVIRONMENT & SETUP\n",
        "# Purpose: Mount Drive and define project paths.\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define Paths\n",
        "# UPDATE THIS if your folder name is different\n",
        "PROJECT_PATH = '/content/drive/MyDrive/AML_FinalProject'\n",
        "IMAGE_DIR = os.path.join(PROJECT_PATH, 'VQA_RAD Image Folder')\n",
        "JSON_FILE = os.path.join(PROJECT_PATH, 'VQA_RAD Dataset Public.json')\n",
        "MODEL_SAVE_PATH = os.path.join(PROJECT_PATH, 'cnn_lstm_vqa_split.pth')\n",
        "\n",
        "# 3. Device Config\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‚úÖ Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vocabulary & Dataset"
      ],
      "metadata": {
        "id": "27gafI20fMQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 2: DATA PIPELINE (UPDATED WITH TRAIN/TEST SPLIT)\n",
        "# Purpose: Build Vocabulary and Custom Dataset Class.\n",
        "# ==============================================================================\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# 1. Vocabulary Class (Unchanged)\n",
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.idx2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.idx = 4\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        return self.word2idx.get(word, self.word2idx[\"<UNK>\"])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "def build_vocab(json_path, threshold=1):\n",
        "    df = pd.read_json(json_path)\n",
        "    counter = Counter()\n",
        "    for question in df['question']:\n",
        "        tokens = nltk.tokenize.word_tokenize(str(question).lower())\n",
        "        counter.update(tokens)\n",
        "    for answer in df['answer']:\n",
        "        tokens = str(answer).lower().split()\n",
        "        counter.update(tokens)\n",
        "    vocab = Vocabulary()\n",
        "    for word, count in counter.items():\n",
        "        if count >= threshold: vocab.add_word(word)\n",
        "    print(f\"‚úÖ Vocabulary Built. Total Size: {len(vocab)}\")\n",
        "    return vocab\n",
        "\n",
        "# 2. Dataset Class (Unchanged)\n",
        "class VQARADDataset(Dataset):\n",
        "    def __init__(self, json_file, img_dir, vocab, transform=None):\n",
        "        self.data = pd.read_json(json_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        img_name = item['image_name']\n",
        "        if not img_name.endswith('.jpg'): img_name += '.jpg'\n",
        "        image = Image.open(os.path.join(self.img_dir, img_name)).convert('RGB')\n",
        "        if self.transform: image = self.transform(image)\n",
        "        tokens = nltk.tokenize.word_tokenize(str(item['question']).lower())\n",
        "        q_indices = [self.vocab(\"<SOS>\")] + [self.vocab(token) for token in tokens] + [self.vocab(\"<EOS>\")]\n",
        "        max_len = 20\n",
        "        if len(q_indices) < max_len: q_indices += [self.vocab(\"<PAD>\")] * (max_len - len(q_indices))\n",
        "        else: q_indices = q_indices[:max_len]\n",
        "        ans_token = str(item['answer']).lower().split()[0]\n",
        "        label = self.vocab(ans_token)\n",
        "        return image, torch.tensor(q_indices), torch.tensor(label)\n",
        "\n",
        "# 3. SPLIT LOGIC (New!)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "vocab = build_vocab(JSON_FILE)\n",
        "full_dataset = VQARADDataset(JSON_FILE, IMAGE_DIR, vocab, transform)\n",
        "\n",
        "# Split 80% Train / 20% Test\n",
        "# using a fixed seed (42) so the test set is always the same!\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"‚úÖ Data Split: {len(train_dataset)} Training samples | {len(test_dataset)} Test samples\")"
      ],
      "metadata": {
        "id": "NApKMgE6fNBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Architecture"
      ],
      "metadata": {
        "id": "jmcSmEehfQSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 3: MODEL ARCHITECTURE (CNN-LSTM)\n",
        "# Purpose: Define the Visual Encoder and Text Decoder.\n",
        "# ==============================================================================\n",
        "\n",
        "class CNN_LSTM_VQA(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=128, hidden_size=256, num_layers=1):\n",
        "        super(CNN_LSTM_VQA, self).__init__()\n",
        "\n",
        "        # 1. Visual Encoder (ResNet50)\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        modules = list(resnet.children())[:-1] # Remove FC layer\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.visual_fc = nn.Linear(2048, hidden_size) # Project to LSTM dimension\n",
        "\n",
        "        # 2. Question Encoder (LSTM)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # 3. Classifier\n",
        "        self.classifier = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, images, questions):\n",
        "        # A. Image Features\n",
        "        features = self.resnet(images)\n",
        "        features = features.view(features.size(0), -1) # Flatten (Batch, 2048)\n",
        "        img_embedding = self.visual_fc(features)       # (Batch, 256)\n",
        "\n",
        "        # B. Text Features\n",
        "        embeds = self.embedding(questions)             # (Batch, Seq, 128)\n",
        "        _, (hidden, _) = self.lstm(embeds)             # Get final hidden state\n",
        "        txt_embedding = hidden[-1]                     # (Batch, 256)\n",
        "\n",
        "        # C. Fusion (Element-wise Multiplication)\n",
        "        fused = img_embedding * txt_embedding\n",
        "\n",
        "        # D. Prediction\n",
        "        logits = self.classifier(fused)\n",
        "        return logits\n",
        "\n",
        "# Initialize Model\n",
        "model = CNN_LSTM_VQA(vocab_size=len(vocab)).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "djVms9DkfRtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "2XGbH3btfVkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 4: TRAINING (Run if retraining is needed)\n",
        "# ==============================================================================\n",
        "\n",
        "def train_model(model, loader, epochs=20):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    model.train()\n",
        "    print(f\"üöÄ Starting Training for {epochs} epochs...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        # Use the passed loader (which will be train_loader)\n",
        "        for imgs, qs, labels in loader:\n",
        "            imgs, qs, labels = imgs.to(device), qs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs, qs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(loader)\n",
        "        loss_history.append(avg_loss)\n",
        "        print(f\"   Epoch [{epoch+1}/{epochs}] Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    torch.save({\n",
        "        'state_dict': model.state_dict(),\n",
        "        'vocab': vocab.word2idx,         # Save vocab dict\n",
        "        'answer_to_idx': vocab.word2idx  # Save map\n",
        "    }, MODEL_SAVE_PATH)\n",
        "    print(f\"üíæ Model Saved to {MODEL_SAVE_PATH}\")\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.plot(loss_history)\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "# TRAIN ON TRAIN_LOADER ONLY\n",
        "# Re-initialize model first to start fresh\n",
        "model = CNN_LSTM_VQA(vocab_size=len(vocab)).to(device) # Note: Cell 3 definition\n",
        "train_model(model, train_loader, epochs=20)"
      ],
      "metadata": {
        "id": "S2x6DvrufZXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation & Visualization"
      ],
      "metadata": {
        "id": "jyCosEzNfbcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 5: EVALUATION & VISUALIZATION\n",
        "# Purpose: Calculate Accuracy and visualize Predictions.\n",
        "# ==============================================================================\n",
        "\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    print(\"üîé Evaluating on TEST SET...\")\n",
        "    with torch.no_grad():\n",
        "        for imgs, qs, labels in loader:\n",
        "            imgs, qs, labels = imgs.to(device), qs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(imgs, qs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    print(f\"‚úÖ Final Test Accuracy: {acc:.2f}%\")\n",
        "    return acc\n",
        "\n",
        "# --- 1. Load Weights Correctly (Handle Dictionary) ---\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "    checkpoint = torch.load(MODEL_SAVE_PATH, map_location=device)\n",
        "\n",
        "    # Check if the file contains the new dictionary format or just weights\n",
        "    if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "    else:\n",
        "        model.load_state_dict(checkpoint)\n",
        "\n",
        "    print(\"üìÇ Loaded Pre-trained Weights successfully.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No saved model found. Using current model state.\")\n",
        "\n",
        "# --- 2. Evaluate on TEST LOADER (Not full dataloader) ---\n",
        "evaluate_model(model, test_loader)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Visualization: Qualitative Results (Test Set Only)\n",
        "# ---------------------------------------------------------\n",
        "def visualize_predictions(model, dataset, num_samples=3):\n",
        "    model.eval()\n",
        "    # Handle case where test set is smaller than num_samples\n",
        "    actual_samples = min(len(dataset), num_samples)\n",
        "    indices = np.random.choice(len(dataset), actual_samples, replace=False)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        img, q_tensor, label_tensor = dataset[idx]\n",
        "\n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            output = model(img.unsqueeze(0).to(device), q_tensor.unsqueeze(0).to(device))\n",
        "            pred_idx = output.argmax(1).item()\n",
        "\n",
        "        # Decode\n",
        "        # Note: We need the original 'vocab' object to decode.\n",
        "        # If 'vocab' isn't available globally, we'd need to pass it in.\n",
        "        q_text = \" \".join([vocab.idx2word[t.item()] for t in q_tensor if t.item() not in [0,1,2]])\n",
        "        truth = vocab.idx2word[label_tensor.item()]\n",
        "        pred = vocab.idx2word[pred_idx]\n",
        "\n",
        "        # Plot\n",
        "        ax = plt.subplot(1, 3, i+1)\n",
        "        # Denormalize image for display\n",
        "        img_disp = img.permute(1, 2, 0).numpy()\n",
        "        img_disp = img_disp * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "\n",
        "        plt.imshow(np.clip(img_disp, 0, 1))\n",
        "        plt.title(f\"Q: {q_text}\\nTrue: {truth} | Pred: {pred}\", fontsize=10)\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Color coding title\n",
        "        if truth == pred:\n",
        "            ax.set_title(ax.get_title(), color='green', fontweight='bold')\n",
        "        else:\n",
        "            ax.set_title(ax.get_title(), color='red')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- 3. Visualize TEST DATASET (Not full dataset) ---\n",
        "visualize_predictions(model, test_dataset)"
      ],
      "metadata": {
        "id": "dHqC2bjDfdg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FINAL EVALUATION CELL: CNN-LSTM COMPARISON METRICS\n",
        "To compare with LLaVA-MED model"
      ],
      "metadata": {
        "id": "ZKbkKx9HP8mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# FINAL EVALUATION CELL: CNN-LSTM COMPARISON METRICS\n",
        "# ==========================================\n",
        "\n",
        "# 1. Install Metrics Libraries (if not already installed)\n",
        "!pip install rouge-score nltk sacrebleu\n",
        "!pip install bert_score\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score  # <--- NEW IMPORT\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms # Ensure transforms is imported\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "class SimpleCNN_LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=256, hidden_size=512):\n",
        "        super(SimpleCNN_LSTM, self).__init__()\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        self.resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        self.visual_fc = nn.Linear(2048, hidden_size)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.classifier = nn.Linear(hidden_size, vocab_size) # Simple Classifier\n",
        "\n",
        "    def forward(self, images, questions):\n",
        "        features = self.resnet(images).view(images.size(0), -1)\n",
        "        img_features = self.visual_fc(features)\n",
        "        embeds = self.embedding(questions)\n",
        "        _, (hidden, _) = self.lstm(embeds)\n",
        "        txt_features = hidden[-1]\n",
        "        fused = img_features * txt_features # Multiplication Fusion\n",
        "        return self.classifier(fused)\n",
        "\n",
        "class ComplexCNN_LSTM(nn.Module):\n",
        "    def __init__(self, vocab_in, vocab_out, embed_size=256, hidden_size=512):\n",
        "        super(ComplexCNN_LSTM, self).__init__()\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        self.resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        self.vision_linear = nn.Linear(2048, hidden_size)\n",
        "        self.bn_vision = nn.BatchNorm1d(hidden_size)\n",
        "        self.embedding = nn.Embedding(vocab_in, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(hidden_size, vocab_out)\n",
        "        )\n",
        "\n",
        "    def forward(self, images, questions):\n",
        "        features = self.resnet(images).view(images.size(0), -1)\n",
        "        img_features = self.bn_vision(self.vision_linear(features))\n",
        "        embeds = self.embedding(questions)\n",
        "        _, (hidden, _) = self.lstm(embeds)\n",
        "        txt_features = hidden[-1]\n",
        "        combined = torch.cat((img_features, txt_features), dim=1) # Concat Fusion\n",
        "        return self.classifier(combined)\n",
        "\n",
        "def evaluate_test_set_complete(model_path, json_path, img_dir):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"--> Loading model from {model_path}...\")\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        print(\"‚ùå Error: Model file not found.\")\n",
        "        return\n",
        "\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint\n",
        "\n",
        "    # 1. Recover Vocab\n",
        "    raw_vocab = checkpoint['vocab']\n",
        "    vocab_wrapper = lambda x: raw_vocab.get(x, raw_vocab.get('<UNK>', 0))\n",
        "    vocab_wrapper.word2idx = raw_vocab\n",
        "\n",
        "    # 2. SMART ARCHITECTURE DETECTION\n",
        "    if 'classifier.3.weight' in state_dict:\n",
        "        print(\"üí° Detected Architecture: COMPLEX (Split Vocab, Concat Fusion)\")\n",
        "        vocab_in = state_dict['embedding.weight'].shape[0]\n",
        "        vocab_out = state_dict['classifier.3.weight'].shape[0]\n",
        "        embed_dim = state_dict['embedding.weight'].shape[1]\n",
        "        hidden_dim = state_dict['lstm.weight_hh_l0'].shape[1]\n",
        "\n",
        "        model = ComplexCNN_LSTM(vocab_in, vocab_out, embed_dim, hidden_dim).to(device)\n",
        "        # Fix layer names for complex model\n",
        "        clean_state_dict = {k.replace(\"vision_encoder\", \"resnet\"): v for k, v in state_dict.items()}\n",
        "\n",
        "    elif 'classifier.weight' in state_dict:\n",
        "        print(\"üí° Detected Architecture: SIMPLE (Shared Vocab, Mult Fusion)\")\n",
        "        vocab_size = state_dict['classifier.weight'].shape[0]\n",
        "        embed_dim = state_dict['embedding.weight'].shape[1]\n",
        "        hidden_dim = state_dict['lstm.weight_hh_l0'].shape[1]\n",
        "\n",
        "        model = SimpleCNN_LSTM(vocab_size, embed_dim, hidden_dim).to(device)\n",
        "        # Fix layer names for simple model\n",
        "        clean_state_dict = state_dict # Usually matches direct save\n",
        "    else:\n",
        "        print(\"‚ùå Error: Unknown model structure.\")\n",
        "        return\n",
        "\n",
        "    # Load Weights\n",
        "    try:\n",
        "        model.load_state_dict(clean_state_dict, strict=False)\n",
        "        print(\"‚úÖ Weights loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Warning during loading: {e}\")\n",
        "\n",
        "    # 3. Setup Dataset & SPLIT IT\n",
        "    tfm = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    full_dataset = VQARADDataset(json_path, img_dir, transform=tfm, vocab=vocab_wrapper)\n",
        "\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    _, test_dataset = random_split(full_dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "    print(f\"--> Evaluating on TEST SET ({len(test_dataset)} samples)...\")\n",
        "\n",
        "    idx2ans = {v: k for k, v in raw_vocab.items()}\n",
        "\n",
        "    # 4. Metrics Setup\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    smoothie = SmoothingFunction().method4\n",
        "\n",
        "    closed_correct = 0; closed_total = 0\n",
        "    open_correct = 0;   open_total = 0\n",
        "\n",
        "    open_bleu_scores = []; open_rouge_scores = []\n",
        "    open_pred_texts = []; open_true_texts = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for i in tqdm(range(len(test_dataset))):\n",
        "        try:\n",
        "            original_idx = test_dataset.indices[i]\n",
        "            row = full_dataset.data.iloc[original_idx]\n",
        "            q_type = str(row['answer_type']).upper()\n",
        "\n",
        "            image, question, _ = test_dataset[i]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output = model(image.unsqueeze(0).to(device), question.unsqueeze(0).to(device))\n",
        "                pred_idx = output.argmax(1).item()\n",
        "\n",
        "            pred_text = idx2ans.get(pred_idx, \"unknown\")\n",
        "            true_text = str(row['answer']).lower().strip()\n",
        "            pred_text = str(pred_text).lower().strip()\n",
        "\n",
        "            # --- METRIC LOGIC ---\n",
        "            if q_type == 'CLOSED':\n",
        "                if pred_text == true_text: closed_correct += 1\n",
        "                closed_total += 1\n",
        "\n",
        "            elif q_type == 'OPEN':\n",
        "                if pred_text == true_text: open_correct += 1\n",
        "                open_total += 1\n",
        "\n",
        "                open_bleu_scores.append(sentence_bleu([true_text.split()], pred_text.split(), smoothing_function=smoothie))\n",
        "                open_rouge_scores.append(scorer.score(true_text, pred_text)['rougeL'].fmeasure)\n",
        "                open_pred_texts.append(pred_text)\n",
        "                open_true_texts.append(true_text)\n",
        "\n",
        "        except Exception as e: continue\n",
        "\n",
        "    # 5. Final Calculation\n",
        "    closed_acc = (closed_correct / closed_total * 100) if closed_total > 0 else 0\n",
        "    open_acc = (open_correct / open_total * 100) if open_total > 0 else 0\n",
        "\n",
        "    bert_f1 = 0\n",
        "    if open_pred_texts:\n",
        "        P, R, F1 = score(open_pred_texts, open_true_texts, lang=\"en\", verbose=False)\n",
        "        bert_f1 = F1.mean().item()\n",
        "\n",
        "    print(f\"\\n=== TEST SET METRICS ===\")\n",
        "    print(f\"Closed Accuracy (Yes/No): {closed_acc:.2f}%\")\n",
        "    print(f\"Open Accuracy (Exact):    {open_acc:.2f}%\")\n",
        "    print(f\"Open BLEU-1:              {np.mean(open_bleu_scores):.4f}\")\n",
        "    print(f\"Open ROUGE-L:             {np.mean(open_rouge_scores):.4f}\")\n",
        "    print(f\"Open BERTScore F1:        {bert_f1:.4f}\")\n",
        "\n",
        "    # Save\n",
        "    pd.DataFrame({\n",
        "        \"Metric\": [\"Closed Accuracy\", \"Open Accuracy\", \"Open BLEU-1\", \"Open ROUGE-L\", \"Open BERTScore\"],\n",
        "        \"Value\": [f\"{closed_acc:.2f}%\", f\"{open_acc:.2f}%\", np.mean(open_bleu_scores), np.mean(open_rouge_scores), bert_f1]\n",
        "    }).to_csv(\"cnn_lstm_test_metrics_complete.csv\", index=False)\n",
        "    print(\"‚úÖ Results saved to 'cnn_lstm_test_metrics_complete.csv'\")\n",
        "\n",
        "# RUN\n",
        "evaluate_test_set_complete(\n",
        "    model_path=MODEL_SAVE_PATH,\n",
        "    json_path=JSON_FILE,\n",
        "    img_dir=IMAGE_DIR\n",
        ")"
      ],
      "metadata": {
        "id": "3vIhffdmQGGb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}